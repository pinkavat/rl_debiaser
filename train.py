"""
    train.py

    Takes a hyperparameter/architecture spec and runs the episode loop.
"""

import torch
import time

from rl_agent.actor_critic_wrappers import RLActor, RLCritic
from rl_agent.ddpg_agent import Agent

DEFAULT_EPISODES = 10

DEFAULT_AGENT_MEMORY = 16
DEFAULT_AGENT_SAMPLE = 16

TODO_STATE_SIZE = 2 # TODO Number of items in the 'state'; the input passed to the actor. A TASK SPECIFICATION, rather than a hardcoded thing, ideally.

def run(dataset, target, spec, log_dir = 'logs/'):
    """
        TODO DOCUMENT

        Temp hyperparameters:
            name -- descriptive
            episodes -- default 10

            promising_threshold -- if the EO drops below this, the run terminates immediately, returning True, otherwise it
                                   runs for the number of episodes, returning False.

            steps -- number of data batches generated by the agent per episode, default 128
            agent_batch_size -- batch size for agent, default 32

            actor_learn_start -- default 0 (learn at once)
            target_reset -- default 1 (every ep)
            critic_reset -- default nonexistent (if existent, episode reset)
            repetitions -- default 1 (how many times a data item is flogged thru target) TODO: removed in single-item model

            agent_gamma -- future discount rate, default 0.99
            agent_tau -- softcopy rate, default 0.001
            agent_memory_size
            agent_sample_size

            
            actor_optimizer
            actor_optimizer_params:
                {lr default 1e-3, etc.}
            critic_optimizer
            critic_optimizer_params:
                {lr default 1e-3, etc.}

            actor_core_spec: dictionary. Either:
                {"hidden_layers":[...]} or {"hidden_count":n, "hidden_size":n}

            ditto critic_core_spec.
    """

    run_name = spec.get('name', f"unnamed_{int(time.time() * 1000.0)}")
    log_path = log_dir + run_name + ".csv"
    log(log_path, ['episode', run_name + "_accuracy", run_name + "_fairness", run_name+"_EO", run_name + "_mean_reward", run_name + "_min_reward", run_name + "_max_reward", run_name + "_mean_future_pred_rew"], mode='w')


    print(f"Starting run {run_name}")
    print("{")
    for k, v in spec.items():
        print(f"    {k}: {v},")
    print("}\n")


    # Set up actor and critic models
    class Perceptron(torch.nn.Module):
        def __init__(self, layer_spec):
            super().__init__()

            layers = [f(layer_spec[x], layer_spec[x+1]) # note to self: cure addiction to list comprehensions 
                for x in range(len(layer_spec) - 1) 
                for f in (lambda m,n: torch.nn.Linear(m,n), lambda m,n:torch.nn.PReLU())]
            layers 
            self.perceptron = torch.nn.Sequential(*layers[:-1]) # Trim the last ReLU

        def forward(self, X):
            return self.perceptron(X)


    actor_layer_spec = get_perceptron_spec(spec.get('actor_core_spec', {}), TODO_STATE_SIZE, dataset.data_item_size)
    actor = RLActor(dataset, Perceptron(actor_layer_spec))

    critic_layer_spec = get_perceptron_spec(spec.get('critic_core_spec', {}), TODO_STATE_SIZE + dataset.data_item_size, 1) # TODO: past state add here?
    critic = RLCritic(dataset, Perceptron(critic_layer_spec))

    actor_optimizer_fn = spec.get('actor_optimizer', torch.optim.Adam)
    actor_optimizer = actor_optimizer_fn(actor.parameters(), **spec.get('actor_optimizer_params', {'lr':1e-3}))
    critic_optimizer_fn = spec.get('critic_optimizer', torch.optim.Adam)
    critic_optimizer = critic_optimizer_fn(critic.parameters(), **spec.get('critic_optimizer_params', {'lr':1e-3}))


    # Set up agent
    agent = Agent(dataset, actor, critic, actor_optimizer, critic_optimizer, spec.get("agent_gamma", 0.99), spec.get("agent_tau", 0.001), sample_size=spec.get('agent_sample_size', DEFAULT_AGENT_SAMPLE), memory_size=spec.get('agent_memory_size', DEFAULT_AGENT_MEMORY), device_override='cpu')
    print(f"DDPG Agent loaded on {agent.device} device")


    # Report initial state
    target.reset()
    print(f"\nInitial target accuracy: {target.get_accuracy()}\nInitial target independence: {target.get_independence()}\nInitial target EO violation: {target.get_max_equalized_odds_violation()}")

    # Log initial state
    log(log_path, [0, target.get_accuracy(), target.get_independence(),target.get_max_equalized_odds_violation(), 0.0, 0.0, 0.0, 0.0]) # TODO initial reward meaningful val?


    
    # Enter training loop
    for episode in range(spec.get('episodes', DEFAULT_EPISODES)): # "episode" for RL, not "epoch"
        
        print(f"Episode {episode}:")

        agent.episode_reset()
        if(episode % spec.get('target_reset', 1) == 0):
            target.reset()

        # Critic Resetting, if specified
        if 'critic_reset' in spec:
            if episode % spec['critic_reset'] == 0:
                for layer in agent.critic.children():
                    if hasattr(layer, 'reset_parameters'):
                        layer.reset_parameters()



        # ========== Train ==========

        steps = spec.get('steps', 128)
        batch_size = spec.get('agent_batch_size', 32)
        use_eo = True # TODO parametrize

        reward_log = [] # TODO sub metric
        future_preds_log = [] # Ditto

        min_intraepisode_eo = float('inf')

        # Initial state setup
        prev_metric = 0.0 - target.get_max_equalized_odds_violation() if use_eo else target.get_independence()
        state = torch.tensor([prev_metric, 0.0])

        for step in range(steps):

            states = []
            actions = []
            rewards = []
            next_states = []

            # Generate batches for the agent to learn from, one item at a time
            for i in range(batch_size):
        
                # 1) Agent acts on single state
                action = agent.act_on(state.unsqueeze(0)) # Note that agent expects batch dimension, and returns batched.

                # 2) Environment reacts
                target.train([action]) # TODO repetitions?

                current_metric = 0.0 - target.get_max_equalized_odds_violation() if use_eo else target.get_independence()
                delta_metric = current_metric - prev_metric
                prev_metric = current_metric

                min_intraepisode_eo = min(min_intraepisode_eo, target.get_max_equalized_odds_violation())

                next_state = torch.tensor([current_metric, delta_metric])
                
                # 3) Store observations for batching
                states.append(state)
                actions.append(action.squeeze(0)) # Note we have to unbatch the action to restack it
                rewards.append(torch.tensor([delta_metric]))
                next_states.append(next_state)
                
                # 4) Advance state for the next pass
                state = next_state

                # TODO: poor-quality metric extraction of reward data
                reward_log.append(delta_metric)
                future_preds_log.append(agent.estimate_future_reward(next_state.unsqueeze(0)).item())


            # Batch the observations and pass them to the agent's memory
            agent.observe(torch.stack(states), torch.stack(actions), torch.stack(rewards), torch.stack(next_states))

            # Agent learns, if it's allowed to
            agent.learn_from_memory(train_actor = episode >= spec.get('actor_learn_start', 0))
            

            print_progress_bar(step, steps)
            print(f" {step} / {steps} ", end='')
        print('')



        # ========== Test ===========

        test_eo = target.get_max_equalized_odds_violation(validation=True)

        print(f"\ttest accuracy: {target.get_accuracy(validation=True)}\n\ttest independence: {target.get_independence(validation=True)}\n\ttest EO violation: {test_eo}\n\ttrain EO violation: {target.get_max_equalized_odds_violation(validation=False)}\n\tmin intraepisode EO: {min_intraepisode_eo}\n")
        # Log to file
        mean_reward = float(sum(reward_log)) / float(len(reward_log))
        mean_future_pred = float(sum(future_preds_log)) / float(len(future_preds_log))
        log(log_path, [episode + 1, target.get_accuracy(validation=True), target.get_independence(validation=True), target.get_max_equalized_odds_violation(validation=True), mean_reward, min(reward_log), max(reward_log), mean_future_pred])


        # Checkpoint-save if needful
        if (episode % 10 == 0):
            agent.save_to(f'temp/checkpoints/episode_{episode}.tar')


        # Check if sufficiently good to stop
        if test_eo < spec.get('promising_threshold', float('-inf')):
            return True

    return False # Ran out the episodes



# Progress bar displayer (cribbed from old code)
def print_progress_bar(val, total):
    BAR_WIDTH = 40
    w = (val / float(total)) * BAR_WIDTH
    bar_string = ''.join([' ' if x > w else '=' for x in range(BAR_WIDTH)])
    print(f"\r[{bar_string}]",end='')


# CSV Logfile append
def log(path, items, mode='a'):
    with open(path, mode) as fp:
        fp.write(",".join([str(item) for item in items]))
        fp.write("\n")


# Helper for parsing perceptron core specifications
def get_perceptron_spec(spec, in_n, out_n):
    if 'hidden_layers' in spec:
        # Manual layer specification
        return [in_n, *(spec['hidden_layers']), out_n]
    else:
        # Num and counts given
        hidden_count = spec.get('hidden_count', 2)
        hidden_size = spec.get('hidden_size', 100)
        return [in_n, *[hidden_size for c in range(hidden_count)], out_n]
